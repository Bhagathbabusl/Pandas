{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Advanced Data Analysis\n",
        "- Advanced data analysis involves sophisticated techniques to derive deeper insights from your data. These techniques often require more complex operations and optimizations to handle large datasets efficiently."
      ],
      "metadata": {
        "id": "Ix9Ckuv0pHfl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Window Functions\n",
        "\n",
        "\n",
        "- Window functions, also known as moving or rolling functions, allow for the application of calculations across a sliding window of data points. They are crucial for tasks such as smoothing time series data, calculating rolling averages, and understanding trends over specified intervals. Window functions in pandas include rolling and expanding calculations.\n",
        "\n",
        "  - `Rolling Window Calculations: `Apply a function to a fixed-size window of data that moves across the dataset. Common functions include mean, sum, standard deviation, and more.\n",
        "\n",
        "  -` Expanding Window Calculations: `Apply a function to an expanding window of data that grows as it progresses through the dataset. This is useful for cumulative calculations.\n",
        "\n",
        "\n",
        "### Types of Window Functions:\n",
        "\n",
        "- `Rolling Mean:` Computes the average over a moving window.\n",
        "- `Rolling Sum:` Computes the sum over a moving window.\n",
        "- `Rolling Standard Deviation:` Measures variability over a moving window.\n",
        "- `Expanding Mean:` Computes the cumulative mean from the start to the current point."
      ],
      "metadata": {
        "id": "iH-l2fOKps-X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFXMGSW0ok8t"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data with daily frequency\n",
        "data = {\n",
        "    'Date': pd.date_range(start='2024-01-01', periods=10, freq='D'),\n",
        "    'Value': [100, 110, 120, 130, 140, 150, 160, 170, 180, 190]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Rolling mean with a window size of 3\n",
        "df['Rolling_Mean'] = df['Value'].rolling(window=3).mean()\n",
        "print(df)\n",
        "\n",
        "# Expanding mean\n",
        "df['Expanding_Mean'] = df['Value'].expanding().mean()\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Aggregation\n",
        "\n",
        "\n",
        "- Data aggregation involves summarizing data by applying functions like sum, mean, min, max, or custom functions to groups of data. This is particularly useful in exploratory data analysis to understand trends and patterns. Aggregation functions can be used in conjunction with groupby() to analyze data across different categories.\n",
        "\n",
        "  - `Using agg():` Allows for the application of multiple aggregation functions to one or more columns.\n",
        "  -`Using transform():` Applies a function to each group and returns a DataFrame with the same shape as the input.\n",
        "\n",
        "\n",
        "### Types of Aggregation Functions:\n",
        "\n",
        "- `Sum:` Total sum of values within each group.\n",
        "- `Mean:` Average value within each group.\n",
        "- `Min/Max:` Minimum or maximum value within each group.\n",
        "- `Custom Functions:` User-defined aggregation functions.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "TyCKrQo3qCzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'Name': ['Bhagath', 'Bharath', 'Monika', 'Padhmavathi', 'Bhagath', 'Monika'],\n",
        "    'Age': [25, 30, 35, 28, 25, 40],\n",
        "    'City': ['Bangalore', 'Chennai', 'Hyderabad', 'Chickkaballapur', 'Bangalore', 'Hyderabad']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Aggregation with groupby\n",
        "agg_result = df.groupby('City').agg({\n",
        "    'Age': ['mean', 'max', 'min'],\n",
        "    'Name': 'count'\n",
        "})\n",
        "print(agg_result)\n",
        "\n",
        "# Using transform to normalize Age within each city\n",
        "df['Normalized_Age'] = df.groupby('City')['Age'].transform(lambda x: (x - x.mean()) / x.std())\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "W-2hCLLnqeRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Optimization\n",
        "\n",
        "\n",
        "- performance optimization focuses on improving the efficiency of data processing and reducing memory usage, which is crucial when working with large datasets. Efficient data handling ensures faster execution of operations and can significantly reduce computational costs.\n",
        "\n",
        "  - `Efficient Data Handling:` Involves techniques like chunking, indexing, and optimizing data types. Chunking allows for processing large datasets in smaller segments. Indexing speeds up data retrieval operations. Choosing appropriate data types reduces memory consumption.\n",
        "  - `Memory Management:` Involves techniques like type conversion (e.g., converting int64 to int32), dropping unnecessary columns, and using more memory-efficient data structures.\n",
        "\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "RB6NeHLQqhv5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Create a large dataset\n",
        "data = {\n",
        "    'Name': np.random.choice(['Bhagath', 'Bharath', 'Monika', 'Padhmavathi'], size=1_000_000),\n",
        "    'Age': np.random.randint(20, 60, size=1_000_000),\n",
        "    'City': np.random.choice(['Bangalore', 'Chennai', 'Hyderabad', 'Chickkaballapur'], size=1_000_000)\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Optimize memory by converting data types\n",
        "df['Age'] = df['Age'].astype(np.int32)\n",
        "df['City'] = df['City'].astype('category')\n",
        "\n",
        "# Efficient chunk processing\n",
        "chunk_size = 100_000\n",
        "chunks = pd.read_csv('large_dataset.csv', chunksize=chunk_size)\n",
        "for chunk in chunks:\n",
        "    process(chunk)  # Replace 'process' with actual processing function\n",
        "\n",
        "print(df.info())\n"
      ],
      "metadata": {
        "id": "AzwTZ3gBqs1f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}